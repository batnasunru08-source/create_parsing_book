import requests
from lxml import html
import time
from urllib.parse import urljoin

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
START_URL = 'https://wuxiaworld.ru/apokalipsis-sistema-sinteza-zombi/glava-887-zamuzhem-za-chung-lingom/'
OUTPUT_FILE = '–ê–ø–æ–∫–∞–ª–∏–ø—Å–∏—Å –°–∏—Å—Ç–µ–º–∞ –°–∏–Ω—Ç–µ–∑–∞ –ó–æ–º–±–∏ 887.txt'
DELAY_SECONDS = 1.5
XPATH_NEXT_PAGE = '/html/body/main/div[2]/div/div[1]/a[2]'
XPATH_HEADER = '/html/body/div/div/nav/h1'

# –°—é–¥–∞ –≤–ø–∏—à–µ—à—å —Ñ—Ä–∞–∑—ã —Ä–µ–∫–ª–∞–º—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä:
# "–ß–∏—Ç–∞—Ç—å –Ω–æ–≤–µ–ª–ª—É", "–ü–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å", "–ù–∞—à —Ç–µ–ª–µ–≥—Ä–∞–º"
AD_KEYWORDS = [
    
    "–†–µ–¥–∞–∫—Ç–∏—Ä—É–µ—Ç—Å—è –ß–∏—Ç–∞—Ç–µ–ª—è–º–∏!",
    "–ù–µ—Ç –≥–ª–∞–≤—ã –∏ —Ç.–ø. - –ø–∏—à–∏ –≤ –ö–æ–º–º–µ–Ω—Ç—ã. –ß–∏—Ç–∞—Ç—å –±–µ–∑ —Ä–µ–∫–ª–∞–º—ã –±–µ—Å–ø–ª–∞—Ç–Ω–æ?!",
    "–Ω–∞—à —Ç–µ–ª–µ–≥—Ä–∞–º",
    "–ê–ø–æ–∫–∞–ª–∏–ø—Å–∏—Å: –°–∏—Å—Ç–µ–º–∞ –°–∏–Ω—Ç–µ–∑–∞ –ó–æ–º–±–∏",
    "–†–∞–Ω–æ–±—ç –ù–æ–≤–µ–ª–ª–∞"
    
]

def remove_ads_from_text(text):
    """–£–¥–∞–ª—è–µ—Ç —Å—Ç—Ä–æ–∫–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞."""
    clean_lines = []
    for line in text.split("\n"):
        if not any(key.lower() in line.lower() for key in AD_KEYWORDS):
            clean_lines.append(line)
    return "\n".join(clean_lines)

def get_page_data(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
    }
    print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º: {url}")
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
        return None, None, None

    tree = html.fromstring(response.content)

    # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ <h1>
    header_elements = tree.xpath(XPATH_HEADER)
    header_text = header_elements[0].text_content().strip() if header_elements else '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞'
    print(f"‚ñ∂ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {header_text}")

    # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –≥–ª–∞–≤—ã
    content_div = tree.xpath('//*[@id="js-full-content"]//p')
    content_text = '\n'.join([p.text_content().strip() for p in content_div if p.text_content().strip()])

    # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Ä–µ–∫–ª–∞–º—ã
    content_text = remove_ads_from_text(content_text)

    full_text = f"{header_text}\n\n{content_text}"

    # –°—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
    next_link = tree.xpath(XPATH_NEXT_PAGE)
    next_url = None
    if next_link:
        next_url = next_link[0].get('href')
        if next_url and not next_url.startswith('http'):
            next_url = urljoin(url, next_url)

    return full_text, next_url, header_text

def get_h1_from_url(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
    }
    try:
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            return None
        tree = html.fromstring(response.content)
        header_elements = tree.xpath(XPATH_HEADER)
        return header_elements[0].text_content().strip() if header_elements else None
    except:
        return None

def crawl_all(start_url, output_file):
    current_url = start_url
    with open(output_file, 'w', encoding='utf-8') as f:
        while current_url:
            text, next_url, current_h1 = get_page_data(current_url)

            if not text:
                print("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.")
                break

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç –≥–ª–∞–≤—ã
            f.write(text + "\n\n--- NEXT PAGE ---\n\n")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–π
            if next_url:
                next_h1 = get_h1_from_url(next_url)
                if next_h1 == current_h1:
                    print(f"üõë –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —Ç–µ–∫—É—â–∏–º: '{current_h1}'")
                    print("–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω.")
                    break

            current_url = next_url
            time.sleep(DELAY_SECONDS)

    print(f"\n‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ —Ñ–∞–π–ª: {output_file}")

if __name__ == '__main__':
    crawl_all(START_URL, OUTPUT_FILE)
